{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install UMAP-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0016ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbe17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in clinical data\n",
    "file_path2 = 'clinical_significant_data_CeD_clean7.csv'\n",
    "CeD_cluster = pd.read_csv(file_path2)\n",
    "columns_to_drop = ['PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16',\n",
    "                    'tag.genotype', 'hibag.A.1', 'hibag.A.2', 'hibag.B.1', 'hibag.B.2', 'hibag.C.1', 'hibag.C.2', \n",
    "                    'hibag.DPB1.1', 'hibag.DPB1.2', 'hibag.DQA1.1', 'hibag.DQA1.2', 'hibag.DQB1.1', 'hibag.DQB1.2', \n",
    "                    'hibag.DRB1.1', 'hibag.DRB1.2', 'hibag.genotype', 'hla.la.A.1', 'hla.la.A.2', 'hla.la.B.1', \n",
    "                    'hla.la.B.2', 'hla.la.C.1', 'hla.la.C.2', 'hla.la.DPB1.1', 'hla.la.DPB1.2', 'hla.la.DQA1.1', \n",
    "                    'hla.la.DQA1.2', 'hla.la.DQB1.1', 'hla.la.DQB1.2', 'hla.la.DRB1.1', 'hla.la.DRB1.2', 'hla.la.genotype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CeD_cluster = CeD_cluster.drop(columns=columns_to_drop)\n",
    "file_path2 = '20240628_HLA.csv'\n",
    "HLA = pd.read_csv(file_path2)\n",
    "print(HLA.columns.to_list())\n",
    "\n",
    "filtered_HLA = HLA[['person_id', 'DQ_geno_4', 'genetic_sex', 'diagnosis domain', 'ancestry_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "CeD_cluster = pd.merge(filtered_HLA, CeD_cluster, on='person_id', how='left')\n",
    "file_path3 = '/run/user/220224/gvfs/smb-share:server=129.112.149.250,share=kong%20lab/AoU_CeD/AoU artificail intelligence/Survery and Measurement/NA_counts.csv'\n",
    "# filter where 'CeD' is equal to 2\n",
    "filtered_data = CeD_cluster[CeD_cluster['CeD'] == 2]\n",
    "answers = pd.read_csv('~/Downloads/survey_answers.csv')\n",
    "filtered_data=pd.merge(filtered_data,answers[['person_id','Overall Health: Average Pain 7 Days']],on='person_id').drop('Average.Pain.7.Days',axis=1)\n",
    "# Counting non-NA values for each column in the filtered DataFrame\n",
    "filtered_data.replace(['PMI: Skip','No matching concept'],[np.nan,np.nan],inplace=True)\n",
    "for i in ['6.or.More.Drinks Last year','Average.Daily.Drink.Count','Education.Level','General.Doctor.Visits','Respected.By.Provider','Spoken.To.Medical.Specialist','How.often.listening.to.doctor','Annual.Income','Health.Insurance','Smoke.Frequency']:\n",
    "    filtered_data[i].replace(0,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Health.Insurance'].replace([0,1,2],[np.nan,0,1],inplace=True)\n",
    "filtered_data['Spoken.To.Medical.Specialist'].replace([0,1,2],[np.nan,0,1],inplace=True)\n",
    "non_na_counts = filtered_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcdd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out columns where the non-NA count is less than 100\n",
    "columns_to_keep = non_na_counts[non_na_counts >= 965].index\n",
    "print(columns_to_keep)\n",
    "# Update filtered_data to only include columns with non-NA count >= 100\n",
    "filtered_data = filtered_data[columns_to_keep]\n",
    "df = pd.read_csv(file_path3)\n",
    "# Create a filter to exclude rows where 'To_remove' equals 'drop'\n",
    "filtered_df = df[df['drop'] == 'drop']\n",
    " # Extract the values from the 'columns' column and store them in a list\n",
    "columns_to_drop3 = filtered_df['Column'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.drop(['PH_raw', 'Global_raw','MH_raw','PROMIS..Phyisical.health', 'PROMIS..Mental.health'],axis=1,inplace=True)\n",
    "filtered_data.drop(filtered_data.columns[filtered_data.columns.str.contains('-diagnosis')],axis=1,inplace=True)\n",
    "filtered_data.drop(filtered_data.columns[filtered_data.columns.str.contains('Daughter|Father|Grandparent|Mother|Son|Sibling')&~filtered_data.columns.str.contains('celiac')],axis=1,inplace=True)\n",
    "file_path4 = '/run/user/220224/gvfs/smb-share:server=129.112.149.250,share=kong%20lab/AoU_CeD/AoU survey data/20240610_survey_cats_fdr.csv'\n",
    "surveys = pd.read_csv(file_path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ad872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Ensure filtered_data is correctly defined earlier in your code or loaded from your data source\n",
    "\n",
    "# List of columns to be encoded\n",
    "filtered_data['DQ_geno_4_code'] = filtered_data['DQ_geno_4'].replace(['2.5/2.5', '2.5/2.2','2.5/7.5','2.5/8.1','2.5/X', '2.2/7.5','2.2/2.2', '2.2/8.1', '2.2/X','7.5/7.5', '7.5/8.1','7.5/X','8.1/8.1','8.1/X','X/X'],np.array(range(15))+1)\n",
    "filtered_data['ancestry_pred_code'] = filtered_data['ancestry_pred'].replace(['eur','afr','eas','sas','mid','amr'],np.array(range(6))+1)\n",
    "filtered_data['diagnosis domain_code'] = filtered_data['diagnosis domain'].replace(['survey','ehr','survey+ehr'],np.array(range(3))+1)\n",
    "filtered_data['is_gen_female_code'] = filtered_data['genetic_sex'].replace(['male','female'],np.array(range(2)))\n",
    "filtered_data['is_hispanic'] = filtered_data['ethnicity'].replace(3,1)\n",
    "columns_to_drop = ['DQ_geno_4', 'genetic_sex', 'diagnosis domain', 'ancestry_pred','ethnicity']\n",
    "\n",
    "\n",
    "filtered_data.drop(columns_to_drop, axis=1, inplace=True)\n",
    "print(filtered_data.drop(['person_id','CeD'],axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fada81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Let's assume 'filtered_data' is your DataFrame\n",
    "# Replace NA values with 0\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "filtered_data_imputed = pd.DataFrame(imputer.fit_transform(filtered_data), columns=filtered_data.columns)\n",
    "scaler = StandardScaler()\n",
    "filtered_data_standardized = pd.DataFrame(scaler.fit_transform(filtered_data_imputed), columns=filtered_data.columns)\n",
    "# Use the Elbow Method to find the optimal number of clusters\n",
    "sse = {}\n",
    "for k in range(1, 11):  # Let's test 1 to 10 clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(filtered_data_standardized)\n",
    "    sse[k] = kmeans.inertia_  # Inertia: Sum of distances of samples to their closest cluster center\n",
    "\n",
    "\n",
    "# Plot SSE for each *k*\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "\n",
    "plt.ylabel(\"SSE\")\n",
    "\n",
    "plt.savefig('SSE_plot.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Configure UMAP\n",
    "umap_reducer = UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "umap_embedding = umap_reducer.fit_transform(filtered_data_standardized)\n",
    "\n",
    "# It can be helpful to visualize the result of UMAP before clustering to see the data structure\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], alpha=0.5)\n",
    "plt.title('UMAP Reduction of Filtered Data')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.colorbar()\n",
    "plt.savefig('UMAP_unclustered.pdf',format='pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Means on the UMAP output\n",
    "kmeans = KMeans(n_clusters=6, n_init='auto', random_state=42)\n",
    "umap_clusters = kmeans.fit_predict(umap_embedding)+1\n",
    "\n",
    "# Visualize the clusters formed\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=umap_clusters, cmap='viridis', alpha=0.6)\n",
    "plt.title('Clustered UMAP Plot')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.colorbar()\n",
    "plt.savefig('UMAP.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(filtered_data_standardized, umap_clusters)\n",
    "print(f'The average silhouette_score is: {silhouette_avg}')\n",
    "\n",
    "# UMAP with higher dimensions\n",
    "umap_reducer = UMAP(n_components=3, random_state=42)  # Using higher dimensions for clustering\n",
    "umap_embedding = umap_reducer.fit_transform(filtered_data_standardized)\n",
    "\n",
    "# Cluster and plot again\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "umap_clusters = kmeans.fit_predict(umap_embedding)\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=umap_clusters+1)\n",
    "plt.title(\"UMAP with adjusted parameters\")\n",
    "plt.savefig('UMAP_adjusted.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Adjust UMAP parameters\n",
    "umap_reducer = UMAP(n_components=2, n_neighbors=30, min_dist=0.01, random_state=42)\n",
    "umap_embedding = umap_reducer.fit_transform(filtered_data_standardized)\n",
    "\n",
    "# Apply clustering to the UMAP output\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "clusters = kmeans.fit_predict(umap_embedding)+1\n",
    "\n",
    "# Recalculate silhouette score\n",
    "silhouette_avg = silhouette_score(umap_embedding, clusters)\n",
    "print(f\"Adjusted silhouette score: {silhouette_avg}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc09dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "plt.title('UMAP Clustering with Adjusted Parameters')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.colorbar(label='Cluster ID')\n",
    "plt.savefig('UMAP_adjusted_lower_dim.pdf',format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'clusters' is the array of cluster labels from your final clustering results\n",
    "filtered_data['Cluster'] = clusters\n",
    "filtered_data.to_csv('~/CeD clustering data3_hk.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dba65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('~/CeD clustering data3_hk.csv')\n",
    "\n",
    "# Drop the 'Unnamed' column if it exists and other specified columns\n",
    "columns_to_drop = ['Unnamed: 0', 'person_id', 'PC1', 'PC2', 'PC3']\n",
    "data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "\n",
    "# Define numerical and categorical variables\n",
    "\n",
    "numerical_columns = ['age', 'ALP(U/L)', 'BMI(kg/m2)', 'T(°C)', 'weight(kg)', 'Ca(md/dl)', 'Cholesterol(mg/dl)',  'SCrea(mg/dl)', 'RBC(x10^6/ul)',   'Glu(mg/dl)', 'HR(bpm)', 'Hb(g/l)', 'WBC(10^3/ul)', 'MCHC(g/dl)',  'PLT(10^3/ul)',  'SBP(mmHg)', 'PROMIS.PH', 'PROMIS.MH', 'PROMIS.total','Overall Health: Average Pain 7 Days']\n",
    "\n",
    "categorical_columns = data.columns[~data.columns.isin(numerical_columns)&(data.columns!='CeD')]\n",
    "\n",
    "# Ensure 'Cluster' is treated as a categorical column\n",
    "if 'Cluster' not in categorical_columns:\n",
    "    categorical_columns.append('Cluster')\n",
    "\n",
    "# Exclude 'Cluster' from numerical columns if mistakenly included\n",
    "if 'Cluster' in numerical_columns:\n",
    "    numerical_columns.remove('Cluster')\n",
    "\n",
    "# Standardize numerical data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = data[numerical_columns].copy()  # Create a copy to avoid modifying original data\n",
    "data_standardized[numerical_columns] = scaler.fit_transform(data_standardized[numerical_columns])\n",
    "\n",
    "# Add 'Cluster' back to the standardized data for grouping\n",
    "data_standardized['Cluster'] = data['Cluster']\n",
    "\n",
    "# Compute medians for numerical variables grouped by 'Cluster'\n",
    "cluster_medians = data_standardized.groupby('Cluster')[numerical_columns].median()\n",
    "cluster_medians.rename({'weight(kg)':'Weight(kg)','age':'Age','T(°C)':'Temp(°C)', 'PROMIS.PH':'PROMIS-Physical Health', 'PROMIS.MH':'PROMIS-Mental Health', 'PROMIS.total':'PROMIS-Overall','Overall Health: Average Pain 7 Days':'Pain'},axis=1,inplace=True)\n",
    "# Transpose the result for better visualization (clusters on x-axis, variables on y-axis)\n",
    "cluster_medians_transposed = cluster_medians.T\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cluster_medians_transposed, cmap='coolwarm', cbar_kws={'label': 'Median Standardized Value'})\n",
    "plt.title('Heatmap of Median Values for Standardized Numerical Variables by Cluster')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Numerical Variables')\n",
    "plt.xticks(rotation=0)  # No need to rotate x-ticks as they are cluster numbers\n",
    "plt.savefig('numeric_heatmap.pdf',format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce512277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('~/CeD clustering data3_hk.csv')\n",
    "\n",
    "# Categorize columns\n",
    "binary_columns = []  # Columns with 0 and 1\n",
    "one_na_columns = []  # Columns with only 1 and NaN\n",
    "\n",
    "for col in data.columns:\n",
    "    unique_vals = data[col].dropna().unique()\n",
    "    if set(unique_vals) == {1}:\n",
    "        one_na_columns.append(col)\n",
    "    elif set(unique_vals) <= {0, 1}:\n",
    "        binary_columns.append(col)\n",
    "\n",
    "# Include 'Cluster' for processing\n",
    "data['Cluster'] = 'Cluster ' + data['Cluster'].astype(str)\n",
    "\n",
    "# Initialize DataFrame to store percentages\n",
    "percentages = pd.DataFrame()\n",
    "\n",
    "# Process binary columns\n",
    "for col in binary_columns:\n",
    "    for cluster in data['Cluster'].unique():\n",
    "        cluster_data = data[data['Cluster'] == cluster]\n",
    "        count_of_ones = cluster_data[col].sum()\n",
    "        total_count = cluster_data[col].count()  # Total non-NaN entries\n",
    "        if total_count > 0:\n",
    "            percentages.loc[col, cluster] = (count_of_ones / total_count) * 100\n",
    "        else:\n",
    "            percentages.loc[col, cluster] = 0\n",
    "\n",
    "# Process one_na_columns\n",
    "for col in one_na_columns:\n",
    "    for cluster in data['Cluster'].unique():\n",
    "        cluster_data = data[data['Cluster'] == cluster]\n",
    "        count_of_ones = cluster_data[col].sum()\n",
    "        total_possible = cluster_data[col].size  # Total entries including NaN\n",
    "        if total_possible > 0:\n",
    "            percentages.loc[col, cluster] = (count_of_ones / total_possible) * 100\n",
    "        else:\n",
    "            percentages.loc[col, cluster] = 0\n",
    "\n",
    "percentages.index=percentages.index.str.replace(' - Self','').str.replace('- Self','')\n",
    "percentages.rename({'is_gen_female_code':'genetic female','irritable bowel syndrome (IBS)':'Irritable Bowel Syndrome','is_hispanic':'Hispanic','other condition(s)':'other condition'},inplace=True)\n",
    "percentages.index=[i.title() for i in  percentages.index]\n",
    "percentages=percentages[['Cluster 1', 'Cluster 2', 'Cluster 3','Cluster 4','Cluster 5', 'Cluster 6']]\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(15, len(percentages.index) * 0.5))  # Adjust height based on number of variables\n",
    "sns.heatmap(percentages, annot=False, cmap='viridis', cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Percentage of Patients who Exhibit the Following Categories')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Categorical Variables')\n",
    "plt.xticks(rotation=45)  # Rotate for better readability\n",
    "plt.savefig('binary_heatmap.pdf',format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('CeD clustering data3_hk.csv')\n",
    "data_merge=data[['person_id','Cluster']]\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Unnamed: 0', 'person_id', 'CeD']\n",
    "data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "\n",
    "# Include 'Cluster' for processing\n",
    "data['Cluster'] = 'Cluster ' + (data['Cluster']+1).astype(str)\n",
    "\n",
    "# Identify categorical columns with more than two but fewer than 16 unique values\n",
    "categorical_columns_multi = []\n",
    "for col in categorical_columns:\n",
    "    if col != 'Cluster':  # Exclude the cluster column from this check\n",
    "        unique_vals = data[col].dropna().unique()\n",
    "        if 2 < len(unique_vals):\n",
    "            categorical_columns_multi.append(col)\n",
    "\n",
    "#get original answers\n",
    "#HLA['DQ_geno_4'].replace(['8.1/X','2.2/X', '7.5/7.5','7.5/8.1', '7.5/X','2.2/2.2'],['other','other','other','other','other','other'],inplace=True)\n",
    "clin_answer = pd.read_csv('clinical significant data_CeD_clean.csv')\n",
    "life_qual = pd.read_csv('life_quality_input.csv')\n",
    "life_qual.drop(['race', 'sex_at_birth', 'Overall Health: Average Fatigue 7 Days', 'Overall Health: Emotional Problem 7 Days', 'Overall Health: General Physical Health'],axis=1,inplace=True)\n",
    "survey_answer = pd.read_csv('survey_answers.csv')\n",
    "answers=pd.merge(clin_answer,survey_answer,on='person_id')\n",
    "answers_DQ=pd.merge(HLA[['person_id','DQ_geno_4','ancestry_pred','diagnosis domain']],answers,on='person_id')\n",
    "answers_qual=pd.merge(answers_DQ,life_qual,on='person_id')\n",
    "answers_clust = pd.merge(answers_qual,data_merge,on='person_id')\n",
    "#filter for multi_category_columns\n",
    "answers_plot = answers_clust[['race','sex_at_birth','Alcohol..6.or.More.Drinks.Occurrence','Alcohol..Average.Daily.Drink.Count','Education.Level..Highest.Grade','Health.Advice..General.Doctor.Visits','Health.Advice..Respected.By.Provider','Income..Annual.Income','Overall Health: Average Fatigue 7 Days','Overall Health: Emotional Problem 7 Days','Overall Health: General Physical Health','Overall Health: General Quality','Overall Health: Everyday Activities','Overall Health: General Mental Health','Overall Health: Social Satisfaction',\t'DQ_geno_4','ancestry_pred','diagnosis domain','Cluster']]\n",
    "answers_plot.replace(['PMI: Skip','PMI: Prefer Not To Answer'],[np.nan,np.nan],inplace=True)\n",
    "answers_plot.columns=['Reported Race','Reported Sex','6 or More Drinks Occurrence','Average Daily Drink Count','Highest Grade','General Doctor Visits','Respected By Provider','Annual Income','Average Fatigue 7 Days','Emotional Problem 7 Days','General Physical Health','General Quality','Everyday Activities','General Mental Health','Social Satisfaction',\t'HLA DQ genotype','Genetic Ancestry','Diagnosis Domain','Cluster']\n",
    "for col in answers_plot.columns.drop('Cluster'):\n",
    "    answers_plot[col]=answers_plot[col].str.replace(f'{col}: ','')\n",
    "\n",
    "\n",
    "answers_plot.rename({'Highest Grade':'Education Level','6 or More Drinks Occurrence':'Frequency of having 6 or more drinks'},axis=1,inplace=True)\n",
    "answers_plot.replace('16 or M ore','16 or More',inplace=True)\n",
    "# Define manual levels for columns\n",
    "manual_levels = {\n",
    "    'Frequency of having 6 or more drinks': ['Never In Last Year','Less Than Monthly', 'Monthly', 'Weekly', 'Daily'],\n",
    "    'Average Daily Drink Count': ['1 or 2', '3 or 4', '5 or 6','7 to 9',  '10 or More'],\n",
    "    'Education Level': ['Never Attended', 'One Through Four','Five Through Eight','Nine Through Eleven','Twelve Or GED','College One to Three','College Graduate','Advanced Degree'],\n",
    "    'General Doctor Visits': ['1', '2 to 3','4 to 5','6 to 7','8 to 9','10 to 12','13 to 15','16 or More'],\n",
    "    'Respected By Provider': [ 'None Of The Time', 'Some Of The Time','Most Of The Time','Always'],\n",
    "    'Annual Income':['10k 25k', '25k 35k','35k 50k','50k 75k', '75k 100k', '100k 150k','150k 200k','more 200k'],\n",
    "    'Average Fatigue 7 Days':['None','Mild','Moderate', 'Severe', 'Very Severe'],\n",
    "    'Emotional Problem 7 Days':['Never', 'Rarely','Sometimes','Often', 'Always' ],\n",
    "    'General Physical Health':['Poor','Fair','Good', 'Very Good', 'Excellent'],\n",
    "    'General Quality':['Poor','Fair','Good', 'Very Good', 'Excellent'],\n",
    "    'Everyday Activities':['Not At All', 'A Little','Moderately','Completely','Mostly'],\n",
    "    'General Mental Health':['Poor','Fair','Good', 'Very Good', 'Excellent'],\n",
    "    'Social Satisfaction':['Poor','Fair','Good', 'Very Good', 'Excellent'],\n",
    "    'HLA DQ genotype':['2.5/2.5','2.5/2.2','2.5/7.5','2.5/8.1','2.5/X','2.2/7.5','2.2/2.2','2.2/8.1','2.2/X', '7.5/7.5','7.5/8.1', '7.5/X','8.1/8.1','8.1/X','other','X/X']\n",
    "    # Add more columns and their manual levels as needed\n",
    "}\n",
    "\n",
    "# Process each categorical column\n",
    "for col in answers_plot.columns.drop('Cluster'):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # Compute the frequency of each category within each cluster\n",
    "    category_counts = answers_plot.pivot_table(index='Cluster', columns=col, aggfunc='size', fill_value=0)\n",
    "    # Check if the column has manual levels specified\n",
    "    if col in manual_levels:\n",
    "        # Apply manual levels\n",
    "        ordered_levels = manual_levels[col]\n",
    "        # Ensure the order includes all categories present in the data\n",
    "        ordered_levels = [level for level in ordered_levels if level in category_counts.columns]\n",
    "        category_counts = category_counts[ordered_levels]\n",
    "    else:\n",
    "        # For other columns, sort by total frequency\n",
    "        total_counts = category_counts.sum(axis=0).sort_values(ascending=False)\n",
    "        category_counts = category_counts[total_counts.index]\n",
    "    # Plot a stacked bar chart\n",
    "    if col == 'HLA DQ genotype':\n",
    "        category_counts.plot(kind='bar', stacked=True, ax=ax, colormap='tab20')\n",
    "    else:\n",
    "        category_counts.plot(kind='bar', stacked=True, ax=ax, colormap='tab20')\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('Cluster')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(title=col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=0)  # Keep cluster labels horizontal for readability\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{col}.pdf',format='pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('~/CeD clustering data3_hk.csv')\n",
    "\n",
    "# Categorize columns\n",
    "binary_columns = []  # Columns with 0 and 1\n",
    "one_na_columns = []  # Columns with only 1 and NaN\n",
    "\n",
    "for col in data.columns:\n",
    "    unique_vals = data[col].dropna().unique()\n",
    "    if set(unique_vals) == {1}:\n",
    "        one_na_columns.append(col)\n",
    "    elif set(unique_vals) <= {0, 1}:\n",
    "        binary_columns.append(col)\n",
    "\n",
    "# Convert 'Cluster' column to a formatted string for clear identification and ordering\n",
    "data['Cluster'] = 'Cluster ' + data['Cluster'].astype(str)\n",
    "clusters_order = ['Cluster ' + str(i+1) for i in range(6)]  # Ensures order from Cluster 0 to Cluster 4\n",
    "\n",
    "# Initialize DataFrame to store percentages\n",
    "percentages = pd.DataFrame()\n",
    "\n",
    "# Process binary columns\n",
    "for col in binary_columns:\n",
    "    for cluster in clusters_order:  # Use predefined cluster order\n",
    "        cluster_data = data[data['Cluster'] == cluster]\n",
    "        count_of_ones = cluster_data[col].sum()\n",
    "        total_count = cluster_data[col].count()  # Total non-NaN entries\n",
    "        if total_count > 0:\n",
    "            percentages.loc[col, cluster] = (count_of_ones / total_count) * 100\n",
    "        else:\n",
    "            percentages.loc[col, cluster] = 0\n",
    "\n",
    "# Process one_na_columns\n",
    "for col in one_na_columns:\n",
    "    for cluster in clusters_order:  # Use predefined cluster order\n",
    "        cluster_data = data[data['Cluster'] == cluster]\n",
    "        count_of_ones = cluster_data[col].sum()\n",
    "        total_possible = cluster_data[col].size  # Total entries including NaN\n",
    "        if total_possible > 0:\n",
    "            percentages.loc[col, cluster] = (count_of_ones / total_possible) * 100\n",
    "        else:\n",
    "            percentages.loc[col, cluster] = 0\n",
    "\n",
    "#clean percentage cts\n",
    "percentages.index=percentages.index.str.replace(' - Self','').str.replace('- Self','')\n",
    "percentages.rename({'is_gen_female_code':'genetic female','irritable bowel syndrome (IBS)':'Irritable Bowel Syndrome','is_hispanic':'Hispanic','other condition(s)':'other condition'},inplace=True)\n",
    "percentages.index=[i.title() for i in  percentages.index]\n",
    "percentages=percentages[['Cluster 1', 'Cluster 2', 'Cluster 3','Cluster 4','Cluster 5', 'Cluster 6']]\n",
    "\n",
    "# Standardize the percentage data\n",
    "scaler = StandardScaler()\n",
    "percentages_standardized = pd.DataFrame(scaler.fit_transform(percentages.T).T, index=percentages.index, columns=percentages.columns)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(15, len(percentages_standardized.index) * 0.5))  # Adjust height based on number of variables\n",
    "sns.heatmap(percentages_standardized, annot=False, cmap='viridis', cbar_kws={'label': 'Standardized Mode Percentage'})\n",
    "plt.title('Heatmap of Standardized Mode Percentages for Binary Variables by Cluster')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Categorical Variables')\n",
    "plt.xticks(rotation=45)  # Rotate for better readability\n",
    "plt.savefig('Standardized_binary.pdf',format='pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

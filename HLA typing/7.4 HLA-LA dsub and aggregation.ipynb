{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLA-LA dsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package Import\n",
    "import sys\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining necessary pathways\n",
    "my_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "cram_paths = \"gs://fc-aou-datasets-controlled/pooled/wgs/cram/v7_delta/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this list contains the subset of samples we have typed for our analysis\n",
    "samples_to_type=pd.read_csv(f'{my_bucket}/data/hla_type_test/cram_lists/typed_samples.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Ensuring dsub is up to date\n",
    "!pip3 install --upgrade dsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manifest file to find cram names per sample id\n",
    "!gsutil -u $GOOGLE_PROJECT cp gs://fc-aou-datasets-controlled/v7/wgs/cram/manifest.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cram_manifest = pd.read_csv('manifest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for the sample list\n",
    "cram_manifest_v7 = cram_manifest[cram_manifest['person_id'].isin(samples_to_type[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cram_manifest_v7.reset_index(drop=True,inplace=True)\n",
    "\n",
    "cram_manifest_v7['cram_uri'].to_csv('AoU_test_crams.txt',index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting for running dsub jobs\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NAME = os.getenv('OWNER_EMAIL').split('@')[0].replace('.','-')\n",
    "\n",
    "# Save this Python variable as an environment variable so that its easier to use within %%bash cells.\n",
    "%env USER_NAME={USER_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/HLA_typing.sh\n",
    "\n",
    "set -o pipefail\n",
    "set -o errexit\n",
    "\n",
    "\n",
    "# ---------Required Inputs---------\n",
    "# aou_crams - A .txt file containing gs:// paths to cram samples.\n",
    "\n",
    "# Given a .txt file - get X samples.\n",
    "# For parallel submissions:\n",
    "# - Use a different .txt file per submission.\n",
    "# - Each .txt file can contain a different number of lines\n",
    "#aou_crams_len=$(wc -l < ${aou_crams})\n",
    "aou_crams_len=1\n",
    "echo \"Samples in cramlist: ${aou_crams_len}\"\n",
    "\n",
    "# ---------Required Output---------\n",
    "#filtered_cram_output\n",
    "\n",
    "echo \"GOOGLE_PROJECT: ${GOOGLE_PROJECT}\"\n",
    "echo \"OUTPUT_PATH: ${OUTPUT_PATH}\"\n",
    "echo \"ref_dict: ${ref_dict}\"\n",
    "echo \"ref_fai: ${ref_fai}\"\n",
    "echo \"ref_fasta: ${ref_fasta}\"\n",
    "\n",
    "# Perform runs for x samples.\n",
    "for i in ${aou_crams_1} ${aou_crams_2} ${aou_crams_3} ${aou_crams_4} ${aou_crams_5};\n",
    " do\n",
    "    # These change per iteration\n",
    "    #export aou_cram_reads=$(sed \"${i}!d;q\" \"${aou_crams}\")   # gs:// path to a cram sample\n",
    "    export aou_cram_reads_name=`basename ${i}`  # file_name.cram\n",
    "    export aou_cram_reads_prefix=\"${aou_cram_reads_name%.*}\" # file_name\n",
    "    echo \"aou_cram_reads: ${aou_cram_reads}\"\n",
    "    echo \"aou_cram_reads_name: ${aou_cram_reads_name}\"\n",
    "    echo \"aou_cram_reads_prefix: ${aou_cram_reads_prefix}\"\n",
    "    \n",
    "    type_hla.sh 8 ${i} /usr/local/bin/Homo_sapiens_assembly38.fasta /usr/local/bin/HLA-LA/graphs/PRG_MHC_GRCh38_withIMGT &&\n",
    "    ls /usr/local/bin/HLA-LA/working/${aou_cram_reads_prefix};\n",
    "    ls\n",
    "    cp ${aou_cram_reads_prefix}_output* ${OUTPUT_PATH}/\n",
    " done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp /home/jupyter/HLA_typing.sh {my_bucket}/dsub/scripts/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp AoU_test_crams.txt $WORKSPACE_BUCKET/data/hla_type_test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $WORKSPACE_BUCKET/aou_dsub.bash ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dsub: commands run 5 samples at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Get a shorter username to leave more characters for the job name.\n",
    "DSUB_USER_NAME=\"$(echo \"${OWNER_EMAIL}\" | cut -d@ -f1)\"\n",
    "\n",
    "# For AoU RWB projects network name is \"network\".\n",
    "AOU_NETWORK=network\n",
    "AOU_SUBNETWORK=subnetwork\n",
    "\n",
    "# Get all cramlists\n",
    "bashArray=()\n",
    "\n",
    "## ------------------------------------------------ MAKE CHANGES HERE ------------------------------------------\n",
    "#Change the 'done < test_cram_batch.txt' to 'done < AoU_v7_batches.txt' if you want to run across all batches\n",
    "while read line; do\n",
    "  bashArray+=($line)\n",
    "done < AoU_test_crams.txt\n",
    "## -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Length of entire array\n",
    "len_bashArray=${#bashArray[@]}\n",
    "\n",
    "LOWER=0\n",
    "UPPER=3\n",
    "#$len_bashArray\n",
    "MACHINE_TYPE=\"n2-standard-4\"\n",
    "## ------------------------------------------------ MAKE CHANGES HERE ------------------------------------------\n",
    "DATE=2024035\n",
    "BASH_SCRIPT=\"gs://fc-secure-5d2c6afb-811e-4186-af87-8d68408d1816/dsub/scripts/HLA_typing.sh\"\n",
    "## -------------------------------------------------------------------------------------------------------------\n",
    "#aou_crams=\"$(gsutil cat ${bashArray[batch]}| head -1)\"\n",
    "#export aou_cram_reads=$(sed \"${i}!d;q\" \"${aou_crams}\")\n",
    "#export aou_cram_reads_name=`basename ${aou_crams}`\n",
    "#export aou_cram_reads_prefix=\"${aou_cram_reads_name%.*}\"\n",
    "#echo \"${bashArray[4]}\"\n",
    "#echo \"${bashArray[5]}\"\n",
    "array_2=(${bashArray[@]:185:5})\n",
    "echo ${array_2[@]}\n",
    "\n",
    "#for ((batch=$LOWER;batch<$UPPER;batch+=1))\n",
    "#do\n",
    "dsub \\\n",
    "        --provider google-cls-v2 \\\n",
    "        --user-project \"${GOOGLE_PROJECT}\"\\\n",
    "        --project \"${GOOGLE_PROJECT}\"\\\n",
    "        --network \"${AOU_NETWORK}\" \\\n",
    "        --subnetwork \"${AOU_SUBNETWORK}\" \\\n",
    "        --service-account \"$(gcloud config get-value account)\" \\\n",
    "        --user \"${DSUB_USER_NAME}\" \\\n",
    "        --regions us-central1 \\\n",
    "        --logging \"${WORKSPACE_BUCKET}/data/hla_type/logging/\" \\\n",
    "        --min-ram 64 \\\n",
    "        --min-cores 8 \\\n",
    "        --boot-disk-size 65 \\\n",
    "        --disk-size 160 \\\n",
    "        --name \"${JOB_NAME}_v1\" \\\n",
    "        --script \"${BASH_SCRIPT}\" \\\n",
    "        --image 'gcr.io/hla-la/mdaya-hla-la:latest' \\\n",
    "        --output-recursive OUTPUT_PATH=\"${WORKSPACE_BUCKET}/data/hla_type_test/\" \\\n",
    "        --env GOOGLE_PROJECT=${GOOGLE_PROJECT} \\\n",
    "        --input aou_crams_1=\"${array_2[0]}\" \\\n",
    "        --input aou_cram_index_1=\"${array_2[0]}\".crai \\\n",
    "        --input aou_crams_2=\"${array_2[1]}\" \\\n",
    "        --input aou_cram_index_2=\"${array_2[1]}\".crai \\\n",
    "        --input aou_crams_3=\"${array_2[2]}\" \\\n",
    "        --input aou_cram_index_3=\"${array_2[2]}\".crai \\\n",
    "        --input aou_crams_4=\"${array_2[3]}\" \\\n",
    "        --input aou_cram_index_4=\"${array_2[3]}\".crai \\\n",
    "        --input aou_crams_5=\"${array_2[4]}\" \\\n",
    "        --input aou_cram_index_5=\"${array_2[4]}\".crai \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Get a shorter username to leave more characters for the job name.\n",
    "DSUB_USER_NAME=\"$(echo \"${OWNER_EMAIL}\" | cut -d@ -f1)\"\n",
    "\n",
    "# For AoU RWB projects network name is \"network\".\n",
    "AOU_NETWORK=network\n",
    "AOU_SUBNETWORK=subnetwork\n",
    "\n",
    "# Get all cramlists\n",
    "bashArray=()\n",
    "\n",
    "## ------------------------------------------------ MAKE CHANGES HERE ------------------------------------------\n",
    "#Change the 'done < test_cram_batch.txt' to 'done < AoU_v7_batches.txt' if you want to run across all batches\n",
    "while read line; do\n",
    "  bashArray+=($line)\n",
    "done < AoU_test_crams.txt\n",
    "## -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Length of entire array\n",
    "len_bashArray=${#bashArray[@]}\n",
    "\n",
    "LOWER=0\n",
    "UPPER=3\n",
    "#$len_bashArray\n",
    "MACHINE_TYPE=\"n2-standard-4\"\n",
    "## ------------------------------------------------ MAKE CHANGES HERE ------------------------------------------\n",
    "DATE=2024035\n",
    "BASH_SCRIPT=\"gs://fc-secure-5d2c6afb-811e-4186-af87-8d68408d1816/dsub/scripts/HLA_typing.sh\"\n",
    "## -------------------------------------------------------------------------------------------------------------\n",
    "#aou_crams=\"$(gsutil cat ${bashArray[batch]}| head -1)\"\n",
    "#export aou_cram_reads=$(sed \"${i}!d;q\" \"${aou_crams}\")\n",
    "#export aou_cram_reads_name=`basename ${aou_crams}`\n",
    "#export aou_cram_reads_prefix=\"${aou_cram_reads_name%.*}\"\n",
    "#echo \"${bashArray[4]}\"\n",
    "#echo \"${bashArray[5]}\"\n",
    "array_2=(${bashArray[@]:190:5})\n",
    "echo ${array_2[@]}\n",
    "\n",
    "#for ((batch=$LOWER;batch<$UPPER;batch+=1))\n",
    "#do\n",
    "dsub \\\n",
    "        --provider google-cls-v2 \\\n",
    "        --user-project \"${GOOGLE_PROJECT}\"\\\n",
    "        --project \"${GOOGLE_PROJECT}\"\\\n",
    "        --network \"${AOU_NETWORK}\" \\\n",
    "        --subnetwork \"${AOU_SUBNETWORK}\" \\\n",
    "        --service-account \"$(gcloud config get-value account)\" \\\n",
    "        --user \"${DSUB_USER_NAME}\" \\\n",
    "        --regions us-central1 \\\n",
    "        --logging \"${WORKSPACE_BUCKET}/data/hla_type/logging/\" \\\n",
    "        --min-ram 64 \\\n",
    "        --min-cores 8 \\\n",
    "        --boot-disk-size 65 \\\n",
    "        --disk-size 160 \\\n",
    "        --name \"${JOB_NAME}_v1\" \\\n",
    "        --script \"${BASH_SCRIPT}\" \\\n",
    "        --image 'gcr.io/hla-la/mdaya-hla-la:latest' \\\n",
    "        --output-recursive OUTPUT_PATH=\"${WORKSPACE_BUCKET}/data/hla_type_test/\" \\\n",
    "        --env GOOGLE_PROJECT=${GOOGLE_PROJECT} \\\n",
    "        --input aou_crams_1=\"${array_2[0]}\" \\\n",
    "        --input aou_cram_index_1=\"${array_2[0]}\".crai \\\n",
    "        --input aou_crams_2=\"${array_2[1]}\" \\\n",
    "        --input aou_cram_index_2=\"${array_2[1]}\".crai \\\n",
    "        --input aou_crams_3=\"${array_2[2]}\" \\\n",
    "        --input aou_cram_index_3=\"${array_2[2]}\".crai \\\n",
    "        --input aou_crams_4=\"${array_2[3]}\" \\\n",
    "        --input aou_cram_index_4=\"${array_2[3]}\".crai \\\n",
    "        --input aou_crams_5=\"${array_2[4]}\" \\\n",
    "        --input aou_cram_index_5=\"${array_2[4]}\".crai \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Get a shorter username to leave more characters for the job name.\n",
    "DSUB_USER_NAME=\"$(echo \"${OWNER_EMAIL}\" | cut -d@ -f1)\"\n",
    "\n",
    "# For AoU RWB projects network name is \"network\".\n",
    "AOU_NETWORK=network\n",
    "AOU_SUBNETWORK=subnetwork\n",
    "\n",
    "# Get all cramlists\n",
    "bashArray=()\n",
    "\n",
    "## ------------------------------------------------ MAKE CHANGES HERE ------------------------------------------\n",
    "#Change the 'done < test_cram_batch.txt' to 'done < AoU_v7_batches.txt' if you want to run across all batches\n",
    "while read line; do\n",
    "  bashArray+=($line)\n",
    "done < AoU_test_crams.txt\n",
    "## -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Length of entire array\n",
    "len_bashArray=${#bashArray[@]}\n",
    "\n",
    "LOWER=0\n",
    "UPPER=3\n",
    "#$len_bashArray\n",
    "MACHINE_TYPE=\"n2-standard-4\"\n",
    "## ------------------------------------------------ MAKE CHANGES HERE ------------------------------------------\n",
    "DATE=2024035\n",
    "BASH_SCRIPT=\"gs://fc-secure-5d2c6afb-811e-4186-af87-8d68408d1816/dsub/scripts/HLA_typing.sh\"\n",
    "## -------------------------------------------------------------------------------------------------------------\n",
    "#aou_crams=\"$(gsutil cat ${bashArray[batch]}| head -1)\"\n",
    "#export aou_cram_reads=$(sed \"${i}!d;q\" \"${aou_crams}\")\n",
    "#export aou_cram_reads_name=`basename ${aou_crams}`\n",
    "#export aou_cram_reads_prefix=\"${aou_cram_reads_name%.*}\"\n",
    "#echo \"${bashArray[4]}\"\n",
    "#echo \"${bashArray[5]}\"\n",
    "array_2=(${bashArray[@]:195:5})\n",
    "echo ${array_2[@]}\n",
    "\n",
    "#for ((batch=$LOWER;batch<$UPPER;batch+=1))\n",
    "#do\n",
    "dsub \\\n",
    "        --provider google-cls-v2 \\\n",
    "        --user-project \"${GOOGLE_PROJECT}\"\\\n",
    "        --project \"${GOOGLE_PROJECT}\"\\\n",
    "        --network \"${AOU_NETWORK}\" \\\n",
    "        --subnetwork \"${AOU_SUBNETWORK}\" \\\n",
    "        --service-account \"$(gcloud config get-value account)\" \\\n",
    "        --user \"${DSUB_USER_NAME}\" \\\n",
    "        --regions us-central1 \\\n",
    "        --logging \"${WORKSPACE_BUCKET}/data/hla_type/logging/\" \\\n",
    "        --min-ram 64 \\\n",
    "        --min-cores 8 \\\n",
    "        --boot-disk-size 65 \\\n",
    "        --disk-size 160 \\\n",
    "        --name \"${JOB_NAME}_v1\" \\\n",
    "        --script \"${BASH_SCRIPT}\" \\\n",
    "        --image 'gcr.io/hla-la/mdaya-hla-la:latest' \\\n",
    "        --output-recursive OUTPUT_PATH=\"${WORKSPACE_BUCKET}/data/hla_type_test/\" \\\n",
    "        --env GOOGLE_PROJECT=${GOOGLE_PROJECT} \\\n",
    "        --input aou_crams_1=\"${array_2[0]}\" \\\n",
    "        --input aou_cram_index_1=\"${array_2[0]}\".crai \\\n",
    "        --input aou_crams_2=\"${array_2[1]}\" \\\n",
    "        --input aou_cram_index_2=\"${array_2[1]}\".crai \\\n",
    "        --input aou_crams_3=\"${array_2[2]}\" \\\n",
    "        --input aou_cram_index_3=\"${array_2[2]}\".crai \\\n",
    "        --input aou_crams_4=\"${array_2[3]}\" \\\n",
    "        --input aou_cram_index_4=\"${array_2[3]}\".crai \\\n",
    "        --input aou_crams_5=\"${array_2[4]}\" \\\n",
    "        --input aou_cram_index_5=\"${array_2[4]}\".crai \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#check status; replace 'jobs' with job id\n",
    "!dstat --provider google-cls-v2 --project terra-vpc-sc-ae994fde --location us-central1 --jobs 'cram-paral--hemanth-karnati--240416-211309-86' --users 'hemanth-karnati' --status '*' -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#collect HLA*LA hla types\n",
    "gsutil ls $WORKSPACE_BUCKET/data/hla_type_test/*/R1_bestguess_G.txt > hla_samples_typed.txt\n",
    "gsutil ls $WORKSPACE_BUCKET/data/hla_type_test/*_output_G.txt >> hla_samples_typed.txt\n",
    "gsutil ls $WORKSPACE_BUCKET/data/hla_type/wgs_*/hla/R1_bestguess_G.txt >> hla_samples_typed.txt\n",
    "\n",
    "wc hla_samples_typed.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formatting HLA-LA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_files = pd.read_csv('hla_samples_typed.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_types = pd.DataFrame()\n",
    "for i in hla_files[0]:\n",
    "    sample = pd.read_csv(i,sep='\\t')\n",
    "    sample['Locus'] = sample['Locus']+'.'+sample['Chromosome'].astype(str)\n",
    "    sample['person_id'] = re.search(r'(wgs_[0-9]+)',i)[1].replace('wgs_','')\n",
    "    type_row = sample.pivot(columns='Locus',values='Allele',index='person_id')\n",
    "    hla_types = pd.concat([hla_types,type_row])\n",
    "\n",
    "hla_types.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_types.to_csv(f'{my_bucket}/data/hla_compare/hla_la_type_table.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_types=pd.read_csv(f'{my_bucket}/data/hla_compare/hla_la_type_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQ genotypes\n",
    "hla_types['Dq2_5_cis'] = ((hla_types['DQA1.1'].str.contains('DQA1[*]05:01') | hla_types['DQA1.2'].str.contains('DQA1[*]05:01')) & \\\n",
    "                  (hla_types['DQB1.1'].str.contains('DQB1[*]02:01') | hla_types['DQB1.2'].str.contains('DQB1[*]02:01'))).astype(int)\n",
    "\n",
    "hla_types['Dq2_5_trans'] = ((hla_types['DQA1.1'].str.contains('DQA1[*]05:05') | hla_types['DQA1.2'].str.contains('DQA1[*]05:05')) & \\\n",
    "                  (hla_types['DQB1.1'].str.contains('DQB1[*]02:02') | hla_types['DQB1.2'].str.contains('DQB1[*]02:02'))).astype(int)\n",
    "\n",
    "hla_types['Dq2_2'] = ((hla_types['DQA1.1'].str.contains('DQA1[*]02:01') | hla_types['DQA1.2'].str.contains('DQA1[*]02:01')) & \\\n",
    "                 (hla_types['DQB1.1'].str.contains('DQB1[*]02:02') | hla_types['DQB1.2'].str.contains('DQB1[*]02:02'))).astype(int)\n",
    "\n",
    "hla_types['Dq7_5'] = ((hla_types['DQA1.1'].str.contains('DQA1[*]05:05') | hla_types['DQA1.2'].str.contains('DQA1[*]05:05')) & \\\n",
    "                 (hla_types['DQB1.1'].str.contains('DQB1[*]03:01') | hla_types['DQB1.2'].str.contains('DQB1[*]03:01'))).astype(int)\n",
    "\n",
    "hla_types['Dq8'] = (((hla_types['DQA1.1'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03')) | (hla_types['DQA1.2'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03'))) & \\\n",
    "                (hla_types['DQB1.1'].str.contains('DQB1[*]03:02') | hla_types['DQB1.2'].str.contains('DQB1[*]03:02'))).astype(int)\n",
    "hla_types.loc[(hla_types['DQA1.1'].str.contains('DQA1[*]05:01') & hla_types['DQA1.2'].str.contains('DQA1[*]05:01')) & \\\n",
    "              (hla_types['DQB1.1'].str.contains('DQB1[*]02:01') & hla_types['DQB1.2'].str.contains('DQB1[*]02:01')),'Dq2_5_1'] = 2\n",
    "\n",
    "hla_types.loc[(hla_types['DQA1.1'].str.contains('DQA1[*]02:01') & hla_types['DQA1.2'].str.contains('DQA1[*]02:01')) & \\\n",
    "              (hla_types['DQB1.1'].str.contains('DQB1[*]02:02') & hla_types['DQB1.2'].str.contains('DQB1[*]02:02')),'Dq2_2'] = 2\n",
    "\n",
    "hla_types.loc[(hla_types['DQA1.1'].str.contains('DQA1[*]05:05') & hla_types['DQA1.2'].str.contains('DQA1[*]05:05')) & \\\n",
    "              (hla_types['DQB1.1'].str.contains('DQB1[*]03:01') & hla_types['DQB1.2'].str.contains('DQB1[*]03:01')),'Dq7_5'] = 2\n",
    "\n",
    "hla_types.loc[((hla_types['DQA1.1'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03')) & (hla_types['DQA1.2'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03'))) & \\\n",
    "              ((hla_types['DQB1.1'].str.contains('DQB1[*]03:02')) & (hla_types['DQB1.2'].str.contains('DQB1[*]03:02'))),'Dq8'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_types[['DQA1.1','DQA1.2','DQB1.1','DQB1.2']]\n",
    "hla_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_types['genotype'] = hla_types.apply(lambda row: \n",
    "    'DQ2.5/DQ2.5' if row['Dq2_5_cis'] == 2 else\n",
    "    'DQ2.2/DQ2.2' if row['Dq2_2'] == 2 else\n",
    "    'DQ7.5/DQ7.5' if row['Dq7_5'] == 2 else\n",
    "    'DQ8/DQ8' if row['Dq8'] == 2 else\n",
    "    'DQ2.5 trans' if row['Dq2_5_trans'] == 1 else\n",
    "    'DQ2.5/DQ2.2' if row['Dq2_5_cis'] == 1 and row['Dq2_2'] == 1 else\n",
    "    'DQ2.5/DQ7.5' if row['Dq2_5_cis'] == 1 and row['Dq7_5'] == 1 else\n",
    "    'DQ2.5/DQ8' if row['Dq2_5_cis'] == 1 and row['Dq8'] == 1 else\n",
    "    'DQ2.5/X' if row['Dq2_5_cis'] == 1 else\n",
    "    'DQ2.2/DQ7.5' if row['Dq2_2'] == 1 and row['Dq7_5'] == 1 else\n",
    "    'DQ2.2/DQ8' if row['Dq2_2'] == 1 and row['Dq8'] == 1 else\n",
    "    'DQ2.2/X' if row['Dq2_2'] == 1 else\n",
    "    'DQ7.5/DQ8' if row['Dq7_5'] == 1 and row['Dq8'] == 1 else\n",
    "    'DQ7.5/X' if row['Dq7_5'] == 1 else\n",
    "    'DQ8/X' if row['Dq8'] == 1 else\n",
    "    'X/X',\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=hla_types[hla_types['genotype']=='X/X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_files['person_id']=[re.search(r'wgs_[0-9]+',i)[0] for i in hla_files[0]]\n",
    "hla_files['person_id']=hla_files['person_id'].str.replace('wgs_','')\n",
    "XX_files = hla_files[hla_files['person_id'].isin(XX['person_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quality checks for typed files\n",
    "k = 0\n",
    "e = 0\n",
    "r = 0\n",
    "q = 0\n",
    "retry_files = []\n",
    "propkmer_error_files = []\n",
    "quality_error_files = []\n",
    "recheck = []\n",
    "for i in hla_files[0]:\n",
    "    df = pd.read_csv(i,sep='\\t')\n",
    "    df.set_index(['Locus','Chromosome'],inplace=True)\n",
    "    if df.loc[('DQA1',1),'perfectG']!=1:\n",
    "        print('file ',i,' not perfectG on DQA1.1; use bestguess')\n",
    "        print('perfectG =', df.loc[('DQA1',1),'perfectG'])\n",
    "        retry_files.append(i)\n",
    "        k+=1\n",
    "    if df.loc[('DQA1',2),'perfectG']!=1:\n",
    "        print('file ',i,' not perfectG on DQA1.2; use bestguess')\n",
    "        print('perfectG =', df.loc[('DQA1',2),'perfectG'])\n",
    "        retry_files.append(i)\n",
    "        k+=1\n",
    "    if df.loc[('DQB1',1),'perfectG']!=1:\n",
    "        print('file ',i,' not perfectG on DQB1.1; use bestguess')\n",
    "        print('perfectG =', df.loc[('DQB1',1),'perfectG'])\n",
    "        retry_files.append(i)\n",
    "        k+=1\n",
    "    if df.loc[('DQB1',2),'perfectG']!=1:\n",
    "        print('file ',i,' not perfectG on DQB1.2; use bestguess')\n",
    "        print('perfectG =', df.loc[('DQB1',2),'perfectG'])\n",
    "        retry_files.append(i)\n",
    "        k+=1\n",
    "    if df.loc[('DQA1',1),'proportionkMersCovered']!=1:\n",
    "        print('file ',i,' has bad proportionkMersCovered on DQA1.1')\n",
    "        print('proportionkMersCovered =', df.loc[('DQA1',1),'proportionkMersCovered'])\n",
    "        propkmer_error_files.append(i) \n",
    "        e+=1\n",
    "    if df.loc[('DQA1',2),'proportionkMersCovered']!=1:\n",
    "        print('file ',i,' has bad proportionkMersCovered on DQA1.2')\n",
    "        print('proportionkMersCovered =', df.loc[('DQA1',2),'proportionkMersCovered'])\n",
    "        propkmer_error_files.append(i) \n",
    "        e+=1\n",
    "    if df.loc[('DQB1',1),'proportionkMersCovered']!=1:\n",
    "        print('file ',i,' has bad proportionkMersCovered on DQB1.1')\n",
    "        print('proportionkMersCovered =', df.loc[('DQB1',1),'proportionkMersCovered'])\n",
    "        propkmer_error_files.append(i) \n",
    "        e+=1\n",
    "    if df.loc[('DQB1',2),'proportionkMersCovered']!=1:\n",
    "        print('file ',i,' has bad proportionkMersCovered on DQB1.2')\n",
    "        print('proportionkMersCovered =', df.loc[('DQB1',2),'proportionkMersCovered'])\n",
    "        propkmer_error_files.append(i) \n",
    "        e+=1\n",
    "    if df.loc[('DQA1',1),'Q1']<=0.99:\n",
    "        print('file ',i,' has bad Quality on DQA1.1')\n",
    "        print('Q1 =', df.loc[('DQA1',1),'Q1'])\n",
    "        quality_error_files.append(i) \n",
    "        q+=1\n",
    "    if df.loc[('DQA1',2),'Q1']<=0.99:\n",
    "        print('file ',i,' has bad Quality on DQA1.2')\n",
    "        print('Q1 =', df.loc[('DQA1',2),'Q1'])\n",
    "        quality_error_files.append(i) \n",
    "        q+=1\n",
    "    if df.loc[('DQB1',1),'Q1']<=0.99:\n",
    "        print('file ',i,' has bad Quality on DQB1.1')\n",
    "        print('Q1 =', df.loc[('DQB1',1),'Q1'])\n",
    "        quality_error_files.append(i) \n",
    "        q+=1\n",
    "    if df.loc[('DQB1',2),'Q1']<=0.99:\n",
    "        print('file ',i,' has bad Quality on DQB1.2')\n",
    "        print('Q1 =', df.loc[('DQB1',2),'Q1'])\n",
    "        quality_error_files.append(i) \n",
    "        q+=1\n",
    "    if df.loc[('DQA1',1),'NColumns_UnaccountedAllele_fGT0.2']!=0:\n",
    "        print('file ',i,' has unaccounted alleles on DQA1.1')\n",
    "        print('NColumns_UnaccountedAllele_fGT0.2 =', df.loc[('DQA1',1),'NColumns_UnaccountedAllele_fGT0'])\n",
    "        error_files.append(i) \n",
    "        r+=1\n",
    "    if df.loc[('DQA1',2),'NColumns_UnaccountedAllele_fGT0.2']!=0:\n",
    "        print('file ',i,' has unaccounted alleles on DQA1.2')\n",
    "        print('NColumns_UnaccountedAllele_fGT0.2 =', df.loc[('DQA1',2),'NColumns_UnaccountedAllele_fGT0.2'])\n",
    "        error_files.append(i) \n",
    "        r+=1\n",
    "    if df.loc[('DQB1',1),'NColumns_UnaccountedAllele_fGT0.2']!=0:\n",
    "        print('file ',i,' has unaccounted alleles on DQB1.1')\n",
    "        print('NColumns_UnaccountedAllele_fGT0.2 =', df.loc[('DQB1',1),'NColumns_UnaccountedAllele_fGT0.2'])\n",
    "        error_files.append(i) \n",
    "        r+=1\n",
    "    if df.loc[('DQB1',2),'NColumns_UnaccountedAllele_fGT0.2']!=0:\n",
    "        print('file ',i,' has unaccounted alleles on DQB1.2')\n",
    "        print('NColumns_UnaccountedAllele_fGT0.2 =', df.loc[('DQB1',2),'NColumns_UnaccountedAllele_fGT0.2'])\n",
    "        error_files.append(i) \n",
    "        r+=1\n",
    "\n",
    "print('retry = ',k)\n",
    "print('low kmer = ',e)\n",
    "print('low quality = ',q)\n",
    "print('recheck = ',r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_G_files=[j.replace('_G','') for j in retry_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check on files where perfectG != 1; results in multiple guesses\n",
    "non_G = pd.DataFrame()\n",
    "for i in non_G_files:\n",
    "    df = pd.read_csv(i,sep='\\t')\n",
    "    df['Locus'] = df['Locus']+'.'+df['Chromosome'].astype(str)\n",
    "    df['person_id']= re.search(r'(wgs_[0-9]+)',i)[1].replace('wgs_','')\n",
    "    type_row = df.pivot(columns='Locus',values='Allele',index='person_id')\n",
    "    non_G = pd.concat([non_G,type_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_G['Dq2_5_1'] = ((non_G['DQA1.1'].str.contains('DQA1[*]05:01') | non_G['DQA1.2'].str.contains('DQA1[*]05:01')) & \\\n",
    "                  (non_G['DQB1.1'].str.contains('DQB1[*]02:01') | non_G['DQB1.2'].str.contains('DQB1[*]02:01'))).astype(int)\n",
    "\n",
    "non_G['Dq2_5_2'] = ((non_G['DQA1.1'].str.contains('DQA1[*]05:05') | non_G['DQA1.2'].str.contains('DQA1[*]05:05')) & \\\n",
    "                  (non_G['DQB1.1'].str.contains('DQB1[*]02:02') | non_G['DQB1.2'].str.contains('DQB1[*]02:02'))).astype(int)\n",
    "\n",
    "non_G['Dq2_2'] = ((non_G['DQA1.1'].str.contains('DQA1[*]02:01') | non_G['DQA1.2'].str.contains('DQA1[*]02:01')) & \\\n",
    "                 (non_G['DQB1.1'].str.contains('DQB1[*]02:02') | non_G['DQB1.2'].str.contains('DQB1[*]02:02'))).astype(int)\n",
    "\n",
    "non_G['Dq7_5'] = ((non_G['DQA1.1'].str.contains('DQA1[*]05:05') | non_G['DQA1.2'].str.contains('DQA1[*]05:05')) & \\\n",
    "                 (non_G['DQB1.1'].str.contains('DQB1[*]03:01') | non_G['DQB1.2'].str.contains('DQB1[*]03:01'))).astype(int)\n",
    "\n",
    "non_G['Dq8'] = (((non_G['DQA1.1'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03')) | (non_G['DQA1.2'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03'))) & \\\n",
    "                (non_G['DQB1.1'].str.contains('DQB1[*]03:02') | non_G['DQB1.2'].str.contains('DQB1[*]03:02'))).astype(int)\n",
    "non_G.loc[(non_G['DQA1.1'].str.contains('DQA1[*]05:01') & non_G['DQA1.2'].str.contains('DQA1[*]05:01')) & \\\n",
    "              (non_G['DQB1.1'].str.contains('DQB1[*]02:01') & non_G['DQB1.2'].str.contains('DQB1[*]02:01')),'Dq2_5_1'] = 2\n",
    "\n",
    "non_G.loc[(non_G['DQA1.1'].str.contains('DQA1[*]02:01') & non_G['DQA1.2'].str.contains('DQA1[*]02:01')) & \\\n",
    "              (non_G['DQB1.1'].str.contains('DQB1[*]02:02') & non_G['DQB1.2'].str.contains('DQB1[*]02:02')),'Dq2_2'] = 2\n",
    "\n",
    "non_G.loc[(non_G['DQA1.1'].str.contains('DQA1[*]05:05') & non_G['DQA1.2'].str.contains('DQA1[*]05:05')) & \\\n",
    "              (non_G['DQB1.1'].str.contains('DQB1[*]03:01') & non_G['DQB1.2'].str.contains('DQB1[*]03:01')),'Dq7_5'] = 2\n",
    "\n",
    "non_G.loc[((non_G['DQA1.1'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03')) & (non_G['DQA1.2'].str.contains('DQA1[*]03:01|DQA1[*]03:02|DQA1[*]03:03'))) & \\\n",
    "              ((non_G['DQB1.1'].str.contains('DQB1[*]03:02')) & (non_G['DQB1.2'].str.contains('DQB1[*]03:02'))),'Dq8'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_G['genotype'] = non_G.apply(lambda row: \n",
    "    'DQ2.5/DQ2.5' if row['Dq2_5_1'] == 2 else\n",
    "    'DQ2.2/DQ2.2' if row['Dq2_2'] == 2 else\n",
    "    'DQ7.5/DQ7.5' if row['Dq7_5'] == 2 else\n",
    "    'DQ8/DQ8' if row['Dq8'] == 2 else\n",
    "    'DQ2.5 trans' if row['Dq2_5_2'] == 1 else\n",
    "    'DQ2.5/DQ2.2' if row['Dq2_5_1'] == 1 and row['Dq2_2'] == 1 else\n",
    "    'DQ2.5/DQ7.5' if row['Dq2_5_1'] == 1 and row['Dq7_5'] == 1 else\n",
    "    'DQ2.5/DQ8' if row['Dq2_5_1'] == 1 and row['Dq8'] == 1 else\n",
    "    'DQ2.5/X' if row['Dq2_5_1'] == 1 else\n",
    "    'DQ2.2/DQ7.5' if row['Dq2_2'] == 1 and row['Dq7_5'] == 1 else\n",
    "    'DQ2.2/DQ8' if row['Dq2_2'] == 1 and row['Dq8'] == 1 else\n",
    "    'DQ2.2/X' if row['Dq2_2'] == 1 else\n",
    "    'DQ7.5/DQ8' if row['Dq7_5'] == 1 and row['Dq8'] == 1 else\n",
    "    'DQ7.5/X' if row['Dq7_5'] == 1 else\n",
    "    'DQ8/X' if row['Dq8'] == 1 else\n",
    "    'X/X',\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_G.to_csv(f'{my_bucket}/data/hla_compare/hla_types_nonperfectG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_types.to_csv(f'{my_bucket}/data/hla_compare/hla_types_hla_la.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_G = pd.read_csv(f'{my_bucket}/data/hla_compare/hla_types_nonperfectG.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now aggregate 3 methods results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_types=pd.read_csv(f'{my_bucket}/data/hla_compare/hla_types_hla_la.csv')\n",
    "hibag_types = pd.read_csv(f'{my_bucket}/data/hibag_hla/hla_types_with_DQ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_tag = pd.read_csv(f'{my_bucket}/data/hla_compare/dq_haplotypes_tag.csv')\n",
    "\n",
    "hla_tag.rename({'haplotype':'genotype'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hibag_types.rename({'sample.id':'person_id'},axis=1,inplace=True)\n",
    "\n",
    "hibag_types.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_la_comp = hla_types[['person_id', 'A.1', 'A.2', 'B.1', 'B.2', 'C.1', 'C.2', 'DPB1.1', 'DPB1.2', 'DQA1.1', 'DQA1.2', 'DQB1.1', 'DQB1.2',\n",
    "       'DRB1.1', 'DRB1.2','genotype']]\n",
    "\n",
    "hla_la_comp.columns = 'hla-la ' + hla_la_comp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_tag_comp = hla_tag[['person_id','genotype']]\n",
    "\n",
    "hla_tag_comp.rename({'genotype':'tag genotype'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hibag_comp=hibag_types[['person_id', 'A.1', 'A.2', 'B.1', 'B.2', 'C.1', 'C.2', 'DPB1.1', 'DPB1.2', 'DQA1.1', 'DQA1.2', 'DQB1.1', 'DQB1.2',\n",
    "       'DRB1.1', 'DRB1.2','genotype']]\n",
    "hibag_comp.columns = 'hibag ' + hibag_comp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hla_la_comp.columns[1:15]:\n",
    "    hla_la_comp[i]=[re.search(r'([0-9]+:[0-9]+)',i)[1] for i in hla_la_comp[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.merge(hla_tag_comp, hibag_comp, left_on='person_id', right_on='hibag person_id')\n",
    "compare = pd.merge(compare,hla_la_comp, left_on='person_id', right_on='hla-la person_id',how='left' )\n",
    "compare.drop(['hla-la person_id','hibag person_id'],axis=1,inplace=True)\n",
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare.to_csv(f'{my_bucket}/data/hla_compare/hla_la_hibag_tag_compare.csv',index=False)\n",
    "!gsutil cp {my_bucket}/data/hla_compare/hla_la_hibag_tag_compare.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
